见：https://blog.csdn.net/jinping_shi/article/details/52433975
和https://blog.csdn.net/zouxy09/article/details/24971995

主要是几何解释比较通俗：

L1特点：Lasso,稀疏正则算子，多维向量的权重向量w中有很多元素必然为0
L2特点：Ridge,防止过拟合，很多w的元素会接近0，但不为0

做回归时（主要是在定义损失函数时），需要用到正则化L1或L2

上述博客通过图形，展示了L1,L2约束下，损失函数求极值问题


L1的主要作用:Lasso正则，得到稀疏权重――――――L1约束下的权值向量w中，很多系数肯定为0，这些系数被认为是无关特征

既然很多系数为0，就可以起到特征筛选的作用，从而简化模型

L2的主要作用：防止过拟合――――得到的权值向量中，参数一般会倾向于更小，因为权值大会导致损失函数变大，从而不是最优。

L2可以起到抗扰动作用：权值越大，模型受影响会更大，所以加入L2模型会更保守，不会随意生成很大的权重系数


通常用法： L1和L2都会加入模型， 他们前面还有个系数α,1-α,表示综合


L1可以启动稀疏作用，是因为，L1的“犄角点”（即在xi=0处不可微）比边界上其他点更容易，接触到损失函数的等值线，这些点的特点（其很多wi必然为0）

L2起不到稀疏编码的作用，通过图形可以看出：L2和Loss函数的交点，在圆的任意位置等概论出现，  但是，它会权衡wi与wj之间的大小关系。   相关性更大的特征，会被赋予更大的权重――如果不加L2约束，很有可能相关性强弱不同的变量，他们的wi却是相同的。

另外还有L0范数――权重向量W中，wi不为0的总数





在spark中由setElasticNetParam（value:Double） 

0<=value<=1，value=0时，只有L2算子，value=1时，只有L1算子，默认=0，即L2正则化