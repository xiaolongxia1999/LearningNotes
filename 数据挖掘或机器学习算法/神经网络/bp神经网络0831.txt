BP神经网络，用图的概念，比较容易理解。 感觉很像一个图的迭代计算。



很清晰的阐述，一次神经网络学习的过程：
---------------强烈推荐
https://blog.csdn.net/dare_kz/article/details/77603522


这篇文章偏理论，有4个等式推导，是BP网络的根本逻辑，可以好好证明一下：
https://blog.csdn.net/u014303046/article/details/78200010

另一篇文章，辅助看看：
https://blog.csdn.net/google19890102/article/details/32723459


再好好看看：

写清晰了，一条样本（不是一批样本）的bp神经网络训练过程：

前项传播――对于开始，即第一条样本数据，根据随机初始权重和输入层，计算隐藏层、输出层的预测值（神经元的线性加权――――激活函数――――得到下一层的某神经元的值）

随机初始权重――――――是对输入层i,隐藏层h,输出层o的每条边，都给一个（-1,1）的随机数，作为每条边的权重


隐藏层i第j个神经元的 预测值为Xij：   Σ （w(i-1）j * X（i-1）m - b(i-1)

此例子中没有引用b，  这个可以认为是偏倚项，权重为-1

然后用激活函数变形：
sigmod用于分类， relu用于回归

最终，一层层计算，得到输出层的值yi*

（yi - yi*）^2 则为 误差e,


e的作用，是用于后面的“后项传播”，通过传播“纠正误差”的方式，更新权重（即改变各条边之间的权重，是他们拟合得更好）。


后项传播，使用到了梯度下降法，可以设定某个学习率a，让误差e最小。


后向传播比较复杂，主要是优化最小e，对各边的权重w求导，利用梯度更新规则，更改各边的w



---从而一次学习过程完成， 此时各边的权重已经更新， 各顶点处的数据全部清理，开始下一条样本的训练

这时，有了权重，再输入层，填下一条数据，即可开始下一次“更新权重”过程

如此迭代计算。



-----------------------------------
-----------------------------------
本质上，是一个迭代计算，每次只计算一条样本。



――――――――――――――
----------------------------
当算法终止时（精度或次数满足条件时）

得到 每条边的“最终权重”。


当你给它一条“预测记录”时，利用“前向传播”的方式，即可得到预测值

与前面不同的是，只是此时不需要“真实值”，因为我们是为了预测，不需要后向传播了。

预测：
输入数据 利用公式  Σ wi * xi - b   ，并用激活函数求出第1层隐藏层的值x1*
隐藏层利用上式（wi和xi都不同了，对应隐藏层的节点） 计算下一层。
最后一个隐藏层，利用上式，计算出输出层的值，  预测结束。

――――――――――――――――――――――――――




但是，这是怎么完成“非线性”的呢？一个非线性的“激活函数”就行？？？


另外，怎么获取最终参数，以及模型形式呢？？？？？


感悟：

隐藏层的神经元――――――可以认为是“抽象特征”

越深的隐藏层，表示越“抽象”的特征
