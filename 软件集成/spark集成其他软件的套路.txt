spark没有自己的存储系统，所以集成，也就是从其他数据源的“读取”、“写入”

而spark由于可以做“离线处理”，“实时处理”，
所以，想实现这2方面的集成，一般要分别对应spark rdd,sql， streaming,structured streaming
作集成。








https://www.jianshu.com/p/a5c669d0ceba

需要三方依赖包：
elasticsearch-spark-13_2.10


https://blog.csdn.net/myproudcodelife/article/details/50985057
spark连接es，从es中读取"index/type",生成一个dataframe
有8种方式


具体的“读取、写入”示例，见：
E:\IdeaWorkSpace\EnergyManagement4\EnergyManagement\examples\test\scala\mydemo\sparkWithEs.scala
亲测有效：


貌似
SparkSesssion.read.format.options(options).load(path)
SparkSesssion.write.format.options(options).save(path)

这两个貌似可以“通用”的读取对应的“数据源”

关键是：三方包中是如何集成“spark”和“数据源”的
options（即配置参数）怎么写

比如es与spark的集成最少配置是：
val options = Map("pushdown"->"true", "es.nodes"->"172.16.32.142","es.port"->"9200")

其它的数据源如mysql等同理